DATA:
  DATASET: cifar100
  DATA_PATH: /nethome/gstoica3/research/pytorch-cifar100/data/cifar-100-python
MODEL:
  TYPE: biswin
  NAME: biswin_tiny_patch4_window7_224_cifar100
  DROP_PATH_RATE: 0.2
  SWIN:
    EMBED_DIM: 96
    DEPTHS: [ 2, 2, 6, 2 ]
    NUM_HEADS: [ 3, 6, 12, 24 ]
    WINDOW_SIZE: 7
    # REVERSE_ATTENTION_LOCATIONS: [0]
    # ALTERED_ATTENTION:
    #   TYPE: 'shared_forward_and_reverse'
    #   REDUCE_REVERSE: True
    #   REVERSE_ACTIVATION: 'gelu'
    #   HYPERNETWORK_BIAS: True
    #   PROJECT_VALUES: True
    #   PROJECT_INPUT: True
    #   VALUE_IS_INPUT: False
    #   TRANSPOSE_SOFTMAX: FALSE
    #   GEN_INDIV_HYPER_WEIGHTS: False
    #   ACTIVATE_HYPER_WEIGHTS: False
    #   ACTIVATE_INPUT: True # Should only be True if ACTIVATE_HYPER_WEIGHTS is False and vice-versa
    #   SINGLE_WEIGHT_MATRIX: False
    #   ENFORCE_ORTHONOGALITY: False
    #   WEIGH_DIRECTIONS: False
    # ALTERED_ATTENTION:
    #   TYPE: 'shared_forward_and_reverse'
    #   REVERSE_ACTIVATION: 'gelu'
    #   HYPERNETWORK_BIAS: True
    #   ENFORCE_ORTHONOGALITY: False
    BIDIRECTIONAL_ATTENTION_LOCATIONS: []
    BIDIRECTION_ATTENTION_APPLY_NORM: False
TRAIN:
  ORTHOGONALITY_LAMBDA: .0
  ALL_ORTHOGONALITY_LAMBDA: .0
  BISA_LAMBDA_REGIME: (0., 600)
  EPOCHS: 600
  WARMUP_EPOCHS: 20
FINETUNING:
  APPLY_FINETUNING: False
  BASE_MODEL: /srv/share4/gstoica3/checkpoints/FineTuneSwinReverseHeadBaseCifarFirstStage/biswin_tiny_patch4_window7_224_cifar100/StopEpoch150DiffNought1/ckpt_epoch_144.pth
  FREEZE_BASE: False
  STOP_EPOCH: 450
  # DIFF_BOUND: .5
  TRAINABLES: head
  BASELINE_USES_SWIN_CFG: False
TUNE_PARAMETERS:
  APPLY_TUNE: False
  WARMUP_EPOCHS: [10, 20, 30]
  WEIGHT_DECAY: [0.05, .1, .2]
  BASE_LR: [5e-4, 3e-4, 1e-4]
  WARMUP_LR: [5e-7, 5e-6]
  MIN_LR: [5e-6]
 